
# Data scaling
scaling: standard

# Training parameters
num_epochs: 500
batch_size: 64
learning_rate: 1e-4
l2_reg: 1e-5
patience: 10

# DeepHit specific parameters
alpha: 1.0
beta: 0.0
gamma: 0.0
h_dim_shared: 64
h_dim_CS: 16
num_layers_shared: 1
num_layers_CS: 2
num_categories: 100
active_fn: tanh

# General parameters
dropout_rate: 0.3
seed: 42
n_folds: 5