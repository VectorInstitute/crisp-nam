

scaling: standard
# Training parameters
num_epochs: 500
batch_size: 256
learning_rate: 1e-3
l2_reg: 1e-5
patience: 10

# DeepHit specific parameters
alpha: 1.0
beta: 0.0
gamma: 0.0
h_dim_shared: 128
h_dim_CS: 16
num_layers_shared: 1
num_layers_CS: 3
num_categories: 100
active_fn: tanh

# General parameters
dropout_rate: 0.3
seed: 42
n_folds: 5